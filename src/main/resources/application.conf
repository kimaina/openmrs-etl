# https://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-batch-processing-times

config {

  input {
    topic: "input"
  }

  output {
    topic: "output"
  }

  stopWords: ["a", "an", "the"]
/// http://www.waitingforcode.com/apache-spark-streaming/window-based-transformations-in-spark-streaming/read
  windowDuration: 30s

  slideDuration: 5s

  spark {
    "spark.master": "local[*]"
    "spark.app.name": "openmrs-etl"
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
    "spark.kryo.registrator": "org.spark.jobs.WordCountKryoRegistration"
  }

  streamingBatchDuration: 5s
  streamingCheckpointDir: ${java.io.tmpdir}

  kafkaSource {
    // kafka brokers
    "metadata.broker.list": "localhost:9092"
    // start from the latest messages (at most once)
    "auto.offset.reset": "largest"
  }

  kafkaSink {
    // kafka bootstrap
    "bootstrap.servers": "localhost:9092"
    // ack from all in-sync replicas
    "acks": "all"
    // reduce buffer size from default 32M to 8M
    "buffer.memory": "8388608"
    // block if buffer is full
    "block.on.buffer.full": "true"
    // retry forever
    "retries": "2147483647"
    "retry.backoff.ms": "1500"
  }
}
